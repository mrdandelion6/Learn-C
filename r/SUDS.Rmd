---
title: "SUDS bootcamp: Analyzing real data" # yaml header
output: 
  html_document:
    code_folding: hide
date: "2024-05-09"
author: "Faisal :)"
---
## Introduction to R
Before we get into data analysis with R, we will go over some fundamental concepts in R. R is an interpreted language like Python, but it is specifically designed for statistical computing. R has strong support for vectorized operations making data visualization, data manipulation, and statistical analysis very easy.

R is often used in an interactive manner where you can run commands in the console and see the output immediately, similiar to Python in interactive mode or Jupyter Notebooks.

## Cleaning up Data
Before we get into our analysis, let's "clean up" our dataset. We can expect that there will be some data-entry errors. Are there any values that _must_ be incorrect? Are there missing values? What about outliers? Should any participants be excluded from our analysis? Can we compute a total test score? Print out a list of missing values that we should double check with the RA.__

Note that any clean up we do to our data must only be inside our code! We should not exclude any data from our original dataset! In other words, the original file should remain untouched.

```{r dataCleaning} 
setwd('./r')
# we will be using CSVs for our data

# note that you will often want to use different packages which can be installed with install.package()
# we comment out this command because we dont wan't our R script to install packages every time we run it
# we could just delete it but it serves as a reminder that we need to install the package, for people who might not have it installed.

# install.packages('knitr')
library('knitr') # this library helps us render our markdown file

# read.csv() reads a CSV provided it's file path.
df <- read.csv("./data.csv", header=TRUE)

# table gives us a summary of our dataframe
table(df)

# INDEXING AND AUTOMATIC ITERATION
# we can use the $ operator to access a column in a dataframe
# a column is also called a variable
# like this: df$variable_name, eg)
df$age

# can summarize columns as well by calling table on it
table(df$age)


# fill in missing values.
df$sex[df$participant_id == "par_018"] <- 'M'

df$age[df$age < 20 | df$age > 50] <- NA

df$cog_score[is.nan(df$cog_score)] <- NA

boxplot(df$brain_score)
boxplot.stats(df$brain_score)

boxplot( df$brain_score[!df$brain_score %in% boxplot.stats(df$brain_score)$out] )

# lets exclude the data in the outliers
df$brain_score[df$brain_score %in% boxplot.stats(df$brain_score)$out] = NA

```



__2. Now that we have clean data, let's check out the basic characteristics of our dataset. Imagine that the PI is concerned that the two participant groups (i.e., patients and controls) are not matched on the basis of age. Let's confirm that age (as a continuous variable) is roughly equal between groups. Let's also check if age is roughly equal if we stratify it into classes of younger and older than 40. Finally, write a few dynamic sentences summarizing our dataset.__

```{r dataSummary}
# install.packages('dplyr')
# install.packages("tidyverse")
library(dplyr) # we use the dplyer library to make a table
library(tidyverse)


# first here's a basic plot
# y ~ x means look at y as a function of x
boxplot(age ~ group, data=df)

# we will now make a table
# we pipe with $>$
df %>%
  group_by(group) %>%
  summarise(
    mean_age = mean(age, na.rm = TRUE),
    median_age = median(age, na.rm = TRUE),
    sd_age = sd(age, na.rm = TRUE)
  ) # the pipe basically splits the variables 

# we now do a t-test to compare the mean_age of our control and the mean_age of our patients
t.test(age ~ group, data=df)

# create a stratified variable for our age group
df$ageGroup <- ifelse(df$age < 40, 'young', 'old') # ifelse(condition, expr_if_true, expr_if_false), just a ternary operator

# we want to turn the ageGroup variable into a Factor type instead of a character because its qualatative so we do:
df$ageGroup <- as.factor( ifelse(df$age < 40, 'true', 'old') ) # returns it as a factor instead

# now we can check for diferences in stratified age between control and patient group
# for differences in qualitative variables, we use chi-squared test
chisq.test(df$group, df$ageGroup) # note that we dont do x ~ y

```

After data cleaning, our data set contains `r nrow(df)` participants.

__3. Now that we have a "clean" dataset, let's examine the primary outcome in the study: `brain_score`. We hypothesized that mean values would differ between patients and controls. Compute group means, create a "publication worthy" plot, and run relevant statistics__. 

```{r mainResult}
# install.packages('ggplot2')
# install.packages('ggpubr')
library(ggplot2)
library(ggpubr)
# we will use an actual plotting library for a fancier plot

# first we do a quick historgram before actually building our fancy plot to just check how our data looks

hist(df$brain_score, breaks = 20)
shapiro.test

# now we make a pretty plot
ggplot(df, aes(x=group, y=brain_score)) +
  geom_boxplot() +
  geom_point(position = position_jitter(width = .2), alpha = 0.5, size = 3) +
  labs(y = 'Brain Score', x = 'Group') +
  scale_x_discrete(label = c('Healthy Controls', "Patients")) +
  scale_fill_brewer(palette = 'Set1') +
  theme_bw() +
  theme(legend.position = 'bottom')
```

__4. Let's explore `brain_score` more thoroughly. Does it correlate with `test_score`? If so, does the group effect of `brain_score` remain after "controlling" for `test_score`?__. 

```{r correlations}
# now we explore variable correlation
# a really good visualization to look for correlation is a scatter plot

df$test_total <- rowSums(df[c('test_1', 'test_2', 'test_3')], na.rm = FALSE)
ggplot(data=df, aes(x=brain_score, y=test_total))



```

__5. In addition to understanding associations between `group` and our variables, it is interesting to know if those variables can "predict" `group`. One way to achieve this is via multivariate clustering (an unsupervised learning method). Let's create clusters via k-means based on brain_score, health_score, and test_score, and see if these agree with diagnosis.__

```{r kmeans}

```

__6. Another interesting question is to look at our data and understand it there's anything else that can help us predict `group`. After all, it's great to collect `brain_score` data, but it's probably expensive and time intensive. Are there any other (combination of) variables in our dataset that can predict if a given participant is a patient or a control? Let's use a decision tree (supervised learning), that allows us to easily integrate factor variables.__

```{r decisionTree}

# libraries
# install.packages('rpart')
# install.packages('caret')
# install.packages('rpart.plot')
library(rpart)
library(caret)
library(rpart.plot)

# set a seed for reproductibility 
set.seed(69)

# split into training and testing
train_indices <- createDataPartition(df$group, p=0.8, list=FALSE)

training_set <- df[train_indices,]
testing_set <- df[-train_indices,]

model <- rpart(group ~ cog_score + health_score + test_total, data=training_set, method='class')
predictions <- predict(model, newdata = testing_set, type='class')
```

```{r}

```

