{"cells":[{"cell_type":"markdown","metadata":{"id":"SF0pPVB4-LHp"},"source":["<a target=\"_blank\" href=\"https://colab.research.google.com/github/upadhyan/SUDS-2024-Bootcamp-Friday/blob/main/Supervised_Learning_Lab.ipynb\">\n","  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n","</a>\n","\n","# UofT DSI SUDS\n","#### Supervised Learning Lab\n","#### Teaching team: Nakul Upadhya\n","##### Lab author: Kyle E. C. Booth, kbooth@mie.utoronto.ca, edited by Jake Mosseri, Nakul Upadhya, Eldan Cohen, Alex Olson, Shehnaz Islam, and Faisal Shaik\n","\n","---\n","#### Note from Faisal (mrdandelion6):\n","I copied this file from a different repository as part of my training in data science at SUDS UofT 2024. Most of the content in these files is not written by me, I have just made some extra edits. Credit to the original authors and editors. You can see the original repository here: https://github.com/upadhyan/SUDS-2024-Bootcamp-Friday\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"R3flbTck-LHu"},"source":["In this lab, we will be introducing *decision tree and forests*. We will introduce the notion of a decision tree, extend this to random forests, and then investigate some state-of-the-art tree-based methods for machine learning. Let's get started!"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"KPXh7nee-LHv"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import sklearn"]},{"cell_type":"markdown","metadata":{"id":"J7CedHdx-LHw"},"source":["### Decision Trees\n","\n","Decision trees are popular supervised learning methods used for classification and regression. The tree represents a series of simple decision rules that predict the target when the feature vector is passed through them. Decision trees are easy to understand, can be visualized nicely, require very little data preparation (e.g., we don't need to scale features), and the trained model can be explained easily to others post priori (as opposed to other *black box* methods that are difficult to communicate).\n","\n","###### Example\n","Suppose you wanted to design a simple decision tree for whether (or not) you buy a used car. You might develop something like the following:\n","\n","<img src=\"https://github.com/lyeskhalil/mlbootcamp/blob/master/img/decision-tree.gif?raw=1\" width=\"500\"/>\n","\n","**YOUR TURN:** Let's say you're browsing Kijiji and come across a used car that: has been road tested, has high mileage, and is a recent year/model.\n","* According to your decision tree model, should you buy this car or not? ____________________________\n","* Will you buy any cars that haven't been road tested (if you follow your model)? ___________________________________\n","\n","Obviously this tree may not be ideal, depending on the situation. For example, you could have a road tested car of a recent year with 2,000,000 km's on it and the model is telling you to buy! (But, you probably shouldn't)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"litVF00HrKud"},"source":["**Titanic Survivor Dataset**\n","\n","In this notebook, we will be exploring the use of decision trees in the context of Kaggle's famous **Titanic dataset**. Each row in the data represents a passenger, detailing various characteristics about them (i.e., the features), and also details whether or not the passenger survived the disaster.\n","\n","Let's load the data and take a look at it.\n","\n","To get the data into a manageable format, we're going to use the [Pandas](https://pandas.pydata.org/) library, a popular library for data manipulation and analysis. For information on Pandas, see [`data_analysis.ipynb`](../data_analysis.ipynb)."]},{"cell_type":"code","execution_count":4,"metadata":{"id":"mDl1JKwx-LHx"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>pclass</th>\n","      <th>survived</th>\n","      <th>name</th>\n","      <th>sex</th>\n","      <th>age</th>\n","      <th>sibsp</th>\n","      <th>parch</th>\n","      <th>ticket</th>\n","      <th>fare</th>\n","      <th>cabin</th>\n","      <th>embarked</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>Allen, Miss. Elisabeth Walton</td>\n","      <td>female</td>\n","      <td>29.0000</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>24160</td>\n","      <td>211.3375</td>\n","      <td>B5</td>\n","      <td>S</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>Allison, Master. Hudson Trevor</td>\n","      <td>male</td>\n","      <td>0.9167</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>113781</td>\n","      <td>151.5500</td>\n","      <td>C22 C26</td>\n","      <td>S</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>Allison, Miss. Helen Loraine</td>\n","      <td>female</td>\n","      <td>2.0000</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>113781</td>\n","      <td>151.5500</td>\n","      <td>C22 C26</td>\n","      <td>S</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>Allison, Mr. Hudson Joshua Creighton</td>\n","      <td>male</td>\n","      <td>30.0000</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>113781</td>\n","      <td>151.5500</td>\n","      <td>C22 C26</td>\n","      <td>S</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>Allison, Mrs. Hudson J C (Bessie Waldo Daniels)</td>\n","      <td>female</td>\n","      <td>25.0000</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>113781</td>\n","      <td>151.5500</td>\n","      <td>C22 C26</td>\n","      <td>S</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   pclass  survived                                             name     sex  \\\n","0       1         1                    Allen, Miss. Elisabeth Walton  female   \n","1       1         1                   Allison, Master. Hudson Trevor    male   \n","2       1         0                     Allison, Miss. Helen Loraine  female   \n","3       1         0             Allison, Mr. Hudson Joshua Creighton    male   \n","4       1         0  Allison, Mrs. Hudson J C (Bessie Waldo Daniels)  female   \n","\n","       age  sibsp  parch  ticket      fare    cabin embarked  \n","0  29.0000      0      0   24160  211.3375       B5        S  \n","1   0.9167      1      2  113781  151.5500  C22 C26        S  \n","2   2.0000      1      2  113781  151.5500  C22 C26        S  \n","3  30.0000      1      2  113781  151.5500  C22 C26        S  \n","4  25.0000      1      2  113781  151.5500  C22 C26        S  "]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["import pandas as pd # import pandas to get access to dataframe operations\n","from sklearn.datasets import fetch_openml # import function to retrieve relevant datasets\n","\n","full_data = fetch_openml(\"titanic\", version=1, as_frame=True) # Get all data and metadata\n","data = full_data.frame # Extract the relevant data\n","data.survived = pd.to_numeric(data['survived'])\n","data.drop(['boat', 'body', 'home.dest'], axis=1, inplace=True) # Drop irrelevant columns\n","data.head() # view the first 5 rows"]},{"cell_type":"markdown","metadata":{"id":"8Ulk4gvl-LHz"},"source":["The above cell used the `fetch_openml` function to pull in the Titanic survivor data from the OpenML website. The `.head()` allows us to conveniently take a glance at the first 5 rows (along with the header). We set `as_frame=True` so fetch_openml returns the data as a Pandas data frame. If `as_frame` is set to False or not specified, the data will be returned as a NumPy `ndarray`.\n","\n","`fetch_openml` will actually return it as a `Bunch` object directly, and you can then pull the data frame from the `.frame` attribute. Hence our `data` variable is a Pandas data frame.\n","\n","We can see that, along with the target 'Survived', we have a number of features including the passenger name, sex, age, fare, cabin, etc. We can do a bit of simple *exploratory data analysis* (EDA) to get a better feel for the dataset."]},{"cell_type":"code","execution_count":5,"metadata":{"id":"yDz5AhGs-LHz"},"outputs":[{"name":"stdout","output_type":"stream","text":["Passengers, features:  (1309, 11)\n","Survived:  500 , Didn't:  809\n","female:  466 , Male:  843\n","\n"," Missing values by feature: \n"," pclass         0\n","survived       0\n","name           0\n","sex            0\n","age          263\n","sibsp          0\n","parch          0\n","ticket         0\n","fare           1\n","cabin       1014\n","embarked       2\n","dtype: int64\n"]}],"source":["print (\"Passengers, features: \", data.shape)\n","print (\"Survived: \", data[data[\"survived\"]==1].shape[0], \", Didn't: \", data[data[\"survived\"]==0].shape[0])\n","# explanation of data[data[\"survived\"]==1].shape[0] syntax:\n","# we use pandas DataFrame filterring to count the number of rows where the \"survived\" column equalls to 1.\n","\n","print (\"female: \", data[data[\"sex\"]==\"female\"].shape[0], \", Male: \", data[data[\"sex\"]==\"male\"].shape[0])\n","print (\"\\n Missing values by feature: \\n\", data.isna().sum())"]},{"cell_type":"markdown","metadata":{"id":"bfTLCpGf-LH0"},"source":["As you can see, we can use Pandas to conveniently summarize key aspects of the dataset such as the number of passengers, features, survived/didn't, and their gender. We are also able to identify the number of missing values per feature in the dataset.\n","\n","To accomplish this, we used Pandas flexible indexing capability. The syntax `data[data[col]==val]` allows us to return the subset of rows in `data` where column `col` takes on value `val`. Very powerful!\n","\n","As you may have suspected, the dataset we're using is actually a subset of the total Titanic data. In reality, there were actually 3,547 passengers while the data we're working with only concerns 1309 of them.\n","\n","**YOUR TURN:**\n","Using similar syntax, answer the following questions about the data:\n","* In the dataset, what is the passenger survival rate? ____________________________\n","* How many passengers paid more than $10 for fare? ____________________________\n","* How many passengers had a passenger class (Pclass) of 3? ________________________\n","* With some discussion/exploration and try to determine what features might be the most relevant to passenger survival."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3qH9urfd-LH1"},"outputs":[],"source":["## Your code here\n","##\n","##"]},{"cell_type":"markdown","metadata":{"id":"LB5y8KEa-LH2"},"source":["##### Data Preparation: Categorical -> Numerical Mapping\n","\n","Before we can fit sklearn decision trees to our data, we first need to convert all of the categorical variables (e.g., gender) numerical values - this is called *encoding*. In previous labs, we dealt with datasets that were pre-prepared; now things are getting a little more realistic! Categoricals with unique values (like name and ticket #) can be removed from the dataset entirely as we don't suspect they will contribute to the model.\n","\n","We can do the required preparation as follows:"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"0WjofYtW-LH2"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>pclass</th>\n","      <th>survived</th>\n","      <th>sex</th>\n","      <th>age</th>\n","      <th>sibsp</th>\n","      <th>parch</th>\n","      <th>fare</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>29.0000</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>211.3375</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0.9167</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>151.5500</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2.0000</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>151.5500</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>30.0000</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>151.5500</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>25.0000</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>151.5500</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   pclass  survived  sex      age  sibsp  parch      fare\n","0       1         1    0  29.0000      0      0  211.3375\n","1       1         1    1   0.9167      1      2  151.5500\n","2       1         0    0   2.0000      1      2  151.5500\n","3       1         0    1  30.0000      1      2  151.5500\n","4       1         0    0  25.0000      1      2  151.5500"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn import preprocessing\n","\n","data = data.drop(['name', 'ticket', 'cabin', 'embarked'], axis=1) # remove unimportant columns\n","\n","le = preprocessing.LabelEncoder() # Create a label encoder\n","le.fit(data['sex']) # provide data for it to learn what classes there are\n","data['sex'] = le.transform(data['sex']) # apply the encoding\n","\n","data.head()"]},{"cell_type":"markdown","metadata":{"id":"Q-a-uiu0-LH3"},"source":["In the above cell, we dropped a number of columns we don't suspect will be correlated with the target (*Note: we probably should have been a bit more rigorous about this!*). Then we used the `LabelEncoder()` within sklearn that can fit a numbering scheme to a categorical feature (i.e., 'Sex'). We can see in the new dataset, sex takes on a value of 0 (female) or 1 (male).\n","\n","##### Model Development\n","\n","OK! Let's get to developing some decision tree models to predict passenger survival. We will start with simple decision trees and develop more complex models from there. Our first step, as in previous labs, is to split our data into a training set and a test set (unseen data). We will then use k-folds cross validation on the training set to try and get the best performing model before finally applying it to the test data.\n","\n","Let's import sklearn's decision tree classifer and split the data (using techniques we covered in the first lab)."]},{"cell_type":"code","execution_count":6,"metadata":{"id":"uXuxpDkP-LH3"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","from sklearn.model_selection import cross_val_score\n","from sklearn import tree # import our decision tree model\n","\n","target_data = data[\"survived\"]\n","feature_data = data.iloc[:, data.columns != \"survived\"]\n","\n","X_train, X_test, y_train, y_test = train_test_split(feature_data, target_data, test_size=0.3, random_state=0)"]},{"cell_type":"markdown","metadata":{},"source":["Note that `random_state` ensures stability by using the same random seed each time."]},{"cell_type":"markdown","metadata":{"id":"l4U5WwH2-LH4"},"source":["**YOUR TURN:**\n","* How many samples are in the training set? _______________________\n","* How many samples are in the test set? _______________________\n","* What are the survival rates in each of the datasets? ______________________"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ANVOauwm-LH4"},"outputs":[],"source":["## Your code here\n","##\n","##\n","##"]},{"cell_type":"markdown","metadata":{"id":"xZp4c8Hg-LH5"},"source":["##### Dealing with Missing Data: Imputation\n","\n","Before we can fit our decision tree to our training data, we can conduct *imputation* to replace missing values with the mean/median/mode value in the column. For this exercise we will conduct mode imputation (i.e., the most common value in the column).\n","\n","**YOUR TURN:** Assuming we have a feature vector with three rows where 'nan' is a missing value:\n","X = [[1, 2, 3],\n","     [1, 2, nan],\n","     [2, 3, 2]]\n","* Which sample has a missing value? (the 1st, 2nd or 3rd?) ______________________________\n","* If we *impute* (i.e., replace the missing value with another value) using the mean (average), what value will go in place of the nan value? __________________________________\n","\n","It's important that you don't impute your data using statistics including the the test data! This is an example of *information leak* where your test data is leaking into your training data.\n","\n","As such, we will fit our missing data imputer to our training data only."]},{"cell_type":"code","execution_count":7,"metadata":{"id":"fQFGk9hk-LH5"},"outputs":[],"source":["from sklearn.impute import SimpleImputer\n","imp = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n","\n","imp.fit(X_train)\n","X_train = imp.transform(X_train) # replace missing data using our imputer"]},{"cell_type":"markdown","metadata":{"id":"mqj-hPeg-LH6"},"source":["##### Fitting a Tree\n","\n","So we've got our data prepared, let's fit a decision tree to our training data.\n","\n","Remember, the pipeline for model development in sklearn is **initialize->fit->predict**."]},{"cell_type":"code","execution_count":10,"metadata":{"id":"VqxAAXVU-LH6"},"outputs":[{"ename":"ValueError","evalue":"could not convert string to float: 'Mellinger, Miss. Madeleine Violet'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[1;32mIn[10], line 6\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tree\n\u001b[0;32m      4\u001b[0m clf \u001b[38;5;241m=\u001b[39m tree\u001b[38;5;241m.\u001b[39mDecisionTreeClassifier(max_depth \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m11\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m \u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m accuracy_score(y_train, clf\u001b[38;5;241m.\u001b[39mpredict(X_train))\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy: \u001b[39m\u001b[38;5;124m\"\u001b[39m, accuracy \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[1;32mc:\\735\\code\\unsynced\\python\\venvs\\learn-to-code\\Lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\735\\code\\unsynced\\python\\venvs\\learn-to-code\\Lib\\site-packages\\sklearn\\tree\\_classes.py:1009\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    978\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    979\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    980\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[0;32m    981\u001b[0m \n\u001b[0;32m    982\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1006\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m   1007\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1009\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1010\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1011\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1012\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1013\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1014\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1015\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n","File \u001b[1;32mc:\\735\\code\\unsynced\\python\\venvs\\learn-to-code\\Lib\\site-packages\\sklearn\\tree\\_classes.py:252\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[1;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    248\u001b[0m check_X_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[0;32m    249\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mDTYPE, accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m, force_all_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    250\u001b[0m )\n\u001b[0;32m    251\u001b[0m check_y_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 252\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate_separately\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcheck_X_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_y_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m missing_values_in_feature_mask \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    257\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_missing_values_in_feature_mask(X)\n\u001b[0;32m    258\u001b[0m )\n\u001b[0;32m    259\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(X):\n","File \u001b[1;32mc:\\735\\code\\unsynced\\python\\venvs\\learn-to-code\\Lib\\site-packages\\sklearn\\base.py:645\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    643\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mestimator\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m check_X_params:\n\u001b[0;32m    644\u001b[0m     check_X_params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdefault_check_params, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_X_params}\n\u001b[1;32m--> 645\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_X_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    646\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mestimator\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m check_y_params:\n\u001b[0;32m    647\u001b[0m     check_y_params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdefault_check_params, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params}\n","File \u001b[1;32mc:\\735\\code\\unsynced\\python\\venvs\\learn-to-code\\Lib\\site-packages\\sklearn\\utils\\validation.py:997\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    995\u001b[0m         array \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(array, dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    996\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 997\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43m_asarray_with_order\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    998\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[0;32m    999\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1000\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[0;32m   1001\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n","File \u001b[1;32mc:\\735\\code\\unsynced\\python\\venvs\\learn-to-code\\Lib\\site-packages\\sklearn\\utils\\_array_api.py:521\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[1;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[0;32m    519\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39marray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    520\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 521\u001b[0m     array \u001b[38;5;241m=\u001b[39m \u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[38;5;66;03m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[0;32m    524\u001b[0m \u001b[38;5;66;03m# container that is consistent with the input's namespace.\u001b[39;00m\n\u001b[0;32m    525\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray(array)\n","\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'Mellinger, Miss. Madeleine Violet'"]}],"source":["from sklearn.metrics import accuracy_score\n","from sklearn import tree\n","\n","clf = tree.DecisionTreeClassifier(max_depth = 11)\n","\n","clf.fit(X_train, y_train)\n","\n","accuracy = accuracy_score(y_train, clf.predict(X_train))\n","print (\"Accuracy: \", accuracy * 100, \"%\")"]},{"cell_type":"markdown","metadata":{"id":"o1yMNmyr-LH6"},"source":["In the above cell, we defined a Decision Tree classifier and fit it to our training set.\n","\n","**YOUR TURN:**\n","* What is the performance of this model on the test set?\n","* Do you think the model has overfit or underfit: ____________\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F4vftzMR-LH7"},"outputs":[],"source":["## Your code here\n","##\n","##\n","##"]},{"cell_type":"markdown","metadata":{"id":"3Eo1zcUlKle0"},"source":["##### Varying Depth\n","\n","Now that we know how to fit a tree, lets see what impact the depth of a tree has on its performance\n","\n","**Your Turn**\n","* Vary the depth of the tree and calculate its train and test accuracy at each depth (Hint: control the maximumum depth using the `max_depth` parameter\n","* Plot the train and test accuracy together.\n","* At what depth is the model underfitting and at what depth is the model overfitting? _______________\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MZhq-FMYKk0n"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","\n","\n","## Your training code here\n","##\n","##\n","##\n","\n","\n","fig, ax = plt.subplots()\n","## Your plotting code here\n","##\n","##\n","##\n","ax.set_title('Model Performance across Depth')\n","fig.show()"]},{"cell_type":"markdown","metadata":{"id":"_HL_BERf-LH8"},"source":["\n","##### Visualizing the Tree\n","\n","One useful thing we can do is actually visualize our decision tree model! We can use the [graphViz](https://www.graphviz.org/) library to accomplish this:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nQ2xJfJm-LH8"},"outputs":[],"source":["from sklearn.tree import export_graphviz\n","import graphviz # Package containing visualization tools\n","\n","clf = tree.DecisionTreeClassifier(max_depth = 3)\n","\n","clf.fit(X_train, y_train)\n","\n","accuracy = accuracy_score(y_train, clf.predict(X_train))\n","print (\"Accuracy: \", accuracy * 100, \"%\")\n","\n","export_graphviz(clf, out_file=\"mytree.dot\", feature_names=data.columns.drop('survived')) # Save the visualization of the tree\n","with open(\"mytree.dot\") as f: # read the file back in\n","    dot_graph = f.read()\n","graphviz.Source(dot_graph) # display the tree"]},{"cell_type":"markdown","metadata":{"id":"3EgolEWB-LH8"},"source":["**YOUR TURN:** Explore the decision tree and answer the following:\n","* What feature does the root node split on?\n","* What features appear most in the tree?"]},{"cell_type":"markdown","metadata":{"id":"T2gqE8y3_yXP"},"source":["##### HP Tuning\n","There are many hyper-parameters that can be tuned to change how the model performs. Some common parameters that are modified include:\n","1. Max Tree Depth: How \"tall\" do you want your tree to be\n","2. Minimum Samples Per Leaf: This parameter defines the minimum number of training datapoints that fall into a given leaf node in order for that node to be created\n","3. Minimum Samples to Split: This parameter controls the minimum number of samples required to create a decision split\n","\n","To decide the values of each of the parameters, we can use Grid Search combined with cross validation. In Grid Search, we first decide what potential values we want each hyperparameter will take. Then we find every possible combination of parameters and run cross validation on each combination to estimate the performance of that hyperparameter combination.\n","\n","Luckily, `sklearn` has a nice implementation of Grid Search that runs this algorithm for us.\n","\n","**Your Turn**\n","\n","Here we want to tune three parameters: max_depth, min_samples_split, and min_samples_leaf. To do this, we need to define possible values we want to search over.\n","* In the cell below, define possible values. NOTE: the more values you specify, the longer grid search will take.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zmpcYAMI_xyX"},"outputs":[],"source":["from sklearn.model_selection import GridSearchCV\n","from sklearn.metrics import make_scorer\n","\n","clf = tree.DecisionTreeClassifier() # First we define our model without passing in parameters\n","hyperparameter_search = { # Then we decide the possible parameter combinations\n","    'max_depth': [], ## FILL THIS IN\n","    'min_samples_split': [2, 5, 8],\n","    'min_samples_leaf': [] ## FILL THIS IN\n","}\n","evaluation_metric = make_scorer(accuracy_score, # GridSearchCV requires us to wrap our metric function in a \"scorer\"\n","                                greater_is_better = True)\n","\n","grid_search_cv = GridSearchCV(estimator = clf,\n","                              param_grid = hyperparameter_search,\n","                              scoring = evaluation_metric,\n","                              cv = 5) # Set up search algorithm\n","grid_search_cv.fit(X_train, y_train) # Run the search. NOTE: This may take a while\n","\n","print(\"Best Parameters: \", grid_search_cv.best_params_) # Print the parameters\n","print (\"Best CV Accuracy: \", grid_search_cv.best_score_ * 100, \"%\")\n","\n","clf = grid_search_cv.best_estimator_ # Get the best model from the GridSearch\n","accuracy = accuracy_score(y_test, clf.predict(imp.transform(X_test)))\n","print (\"Testing Accuracy: \", accuracy * 100, \"%\") # Print the testing accuracy of the best model"]},{"cell_type":"markdown","metadata":{"id":"Qrr1hd59EiVK"},"source":["In the cell above, we tested our three values per hyperparameter and ran grid search to find the best combination from the space we defined. As you may have noticed, the number of combinations tested by Grid Search exponentially increases as you test more values and tune more hyperparameters. This means that performing a grid search is often a task that takes a long period of time and is often **not** used for more complex models like neural networks."]},{"cell_type":"markdown","metadata":{"id":"LlFxKh8YQtnT"},"source":["### Other Models\n","If you have finished early, feel free to try other models and try to get as high of a test accuracy as you can!\n","\n","Some other models you can start with:\n","1. [Logistic Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n","2. [Random Forest](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier)\n","3. [SVM](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)\n","\n","You can also use models from other packages. Some examples include:\n","1. [XGBoost](https://xgboost.readthedocs.io/en/stable/get_started.html) (you will need to run `!pip install xgboost`)\n","2. [LightGBM](https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMClassifier.html#lightgbm.LGBMClassifier) (`!pip install lightgbm`)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rbx6Ut2vQwWI"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.1"}},"nbformat":4,"nbformat_minor":0}
