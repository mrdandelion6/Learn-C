{"cells":[{"cell_type":"markdown","metadata":{"id":"Ui1KpY6XyxFj"},"source":["# A walkthrough of text analysis and TF-IDF\n","#### Material from Google Colab Tutorials\n","#### Edited by Nakul Upadhya and Faisal Shaik\n","\n","---\n","#### Note from Faisal (mrdandelion6):\n","I copied this file from a different repository as part of my training in data science at SUDS UofT 2024. Most of the content in these files is not written by me, I have just made some extra edits. Credit to the original authors and editors. You can see the original repository here: https://github.com/upadhyan/SUDS-2024-Bootcamp-Friday\n","\n","---\n","\n","We'll start by using scikit-learn to count words, then come across some of the issues with simple word count analysis. Most of these problems can be tackled with TF-IDF - a single word might mean less in a longer text, and common words may contribute less to meaning than more rare ones."]},{"cell_type":"code","execution_count":2,"metadata":{"id":"DHYgbsWsyxFk"},"outputs":[],"source":["import pandas as pd\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","import re\n","from nltk.stem.porter import PorterStemmer\n","\n","pd.options.display.max_columns = 30\n","%matplotlib inline"]},{"cell_type":"markdown","metadata":{"id":"QeFIPNJuyxFk"},"source":["## Text analysis refresher\n","\n","Text analysis has a few parts. We are going to use **bag of words** analysis, which just treats a sentence like a bag of words - no particular order or anything. It's simple but it usually gets the job done adequately.\n","\n","Here is our text."]},{"cell_type":"code","execution_count":3,"metadata":{"collapsed":true,"id":"UfFydiXKyxFl"},"outputs":[],"source":["texts = [\n","    \"Penny bought bright blue fishes.\",\n","    \"Penny bought bright blue and orange fish.\",\n","    \"The cat ate a fish at the store.\",\n","    \"Penny went to the store. Penny ate a bug. Penny saw a fish.\",\n","    \"It meowed once at the bug, it is still meowing at the bug and the fish\",\n","    \"The cat is at the fish store. The cat is orange. The cat is meowing at the fish.\",\n","    \"Penny is a fish\"\n","]"]},{"cell_type":"markdown","metadata":{"id":"GSWnar66yxFl"},"source":["When you process text, you have a nice long series of steps, but let's say you're interested in three things:\n","\n","1. **Tokenizing** converts all of the sentences/phrases/etc into a series of words, and then it might also include converting it into a series of numbers - math stuff only works with numbers, not words. So maybe 'cat' is 2 and 'rug' is 4 and stuff like that.\n","2. **Counting** takes those words and sees how many there are (obviously) - how many times does `meow` appear?\n","3. **Normalizing** takes the count and makes new numbers - maybe it's how many times `meow` appears vs. how many total words there are, or maybe you're seeing how often `meow` comes up to see whether it's important."]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1715300363848,"user":{"displayName":"Nakul Upadhya","userId":"08924826005411940959"},"user_tz":240},"id":"QyIkXrXKyxFl","outputId":"90b5b8f9-831c-442e-bc7d-8812a4150f3d"},"outputs":[{"data":{"text/plain":["['Penny', 'bought', 'bright', 'blue', 'fishes']"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["\"Penny bought bright blue fishes\".split()"]},{"cell_type":"markdown","metadata":{"id":"LIXY1vc3yxFl"},"source":["      Penny bought bright blue fishes.\n","\n","If we **tokenize** that sentence, we're just lowercasing it, removing the punctuation and splitting on spaces - `penny bought bright blue fishes`.\n"]},{"cell_type":"markdown","metadata":{"id":"at0TTwbvyxFm"},"source":["The `scikit-learn` package does a **ton of stuff**, some of which includes the above. We're going to start by playing with the `CountVectorizer`."]},{"cell_type":"code","execution_count":5,"metadata":{"collapsed":true,"id":"aEk_MabYyxFm"},"outputs":[],"source":["from sklearn.feature_extraction.text import CountVectorizer\n","count_vectorizer = CountVectorizer()"]},{"cell_type":"code","execution_count":6,"metadata":{"collapsed":true,"id":"30vkcpeNyxFm"},"outputs":[],"source":["# .fit_transform will both TOKENIZE and COUNT\n","X = count_vectorizer.fit_transform(texts)"]},{"cell_type":"markdown","metadata":{"id":"X1zXFdkryxFm"},"source":["Let's take a look at what it found out!"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1715300364136,"user":{"displayName":"Nakul Upadhya","userId":"08924826005411940959"},"user_tz":240},"id":"XkOXEZf5yxFm","outputId":"3c583d64-9f38-4fa1-9ea1-a0eff6da2a5c"},"outputs":[{"data":{"text/plain":["<7x23 sparse matrix of type '<class 'numpy.int64'>'\n","\twith 49 stored elements in Compressed Sparse Row format>"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["X"]},{"cell_type":"markdown","metadata":{"id":"GK05x0uwyxFm"},"source":["Okay, that looks like trash and garbage. What's a \"sparse array\"?????? We see that we can use `.toarray()` on a sparse array to return an `ndarray` from it."]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1715300364136,"user":{"displayName":"Nakul Upadhya","userId":"08924826005411940959"},"user_tz":240},"id":"Km_wjecmyxFm","outputId":"bf4dcfcb-f6c1-4f63-e64a-60bdda97235f"},"outputs":[{"data":{"text/plain":["array([[0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n","        0],\n","       [1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n","        0],\n","       [0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0,\n","        0],\n","       [0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 3, 1, 0, 1, 1, 1,\n","        1],\n","       [1, 2, 0, 0, 0, 0, 2, 0, 1, 0, 1, 2, 1, 1, 1, 0, 0, 0, 1, 0, 3, 0,\n","        0],\n","       [0, 2, 0, 0, 0, 0, 0, 3, 2, 0, 3, 0, 0, 1, 0, 1, 0, 0, 0, 1, 5, 0,\n","        0],\n","       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n","        0]], dtype=int64)"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["X.toarray()"]},{"cell_type":"markdown","metadata":{"id":"4vj7Nmz1yxFn"},"source":["Note that `X.toarray()` returned an ndarray.\n","\n","If we put on our **Computer Goggles** we see that the first sentence has the first word 3 times, the second word 1 time, the third word 1 time, etc... But we can't *read* it, really. It would look nicer as a dataframe and with the words labeled."]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":269},"executionInfo":{"elapsed":201,"status":"ok","timestamp":1715300404402,"user":{"displayName":"Nakul Upadhya","userId":"08924826005411940959"},"user_tz":240},"id":"2DXuAUtLyxFn","outputId":"f2858102-3355-4c45-e02d-c8ff46168791"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>and</th>\n","      <th>at</th>\n","      <th>ate</th>\n","      <th>blue</th>\n","      <th>bought</th>\n","      <th>bright</th>\n","      <th>bug</th>\n","      <th>cat</th>\n","      <th>fish</th>\n","      <th>fishes</th>\n","      <th>is</th>\n","      <th>it</th>\n","      <th>meowed</th>\n","      <th>meowing</th>\n","      <th>once</th>\n","      <th>orange</th>\n","      <th>penny</th>\n","      <th>saw</th>\n","      <th>still</th>\n","      <th>store</th>\n","      <th>the</th>\n","      <th>to</th>\n","      <th>went</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>5</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   and  at  ate  blue  bought  bright  bug  cat  fish  fishes  is  it  meowed  \\\n","0    0   0    0     1       1       1    0    0     0       1   0   0       0   \n","1    1   0    0     1       1       1    0    0     1       0   0   0       0   \n","2    0   1    1     0       0       0    0    1     1       0   0   0       0   \n","3    0   0    1     0       0       0    1    0     1       0   0   0       0   \n","4    1   2    0     0       0       0    2    0     1       0   1   2       1   \n","5    0   2    0     0       0       0    0    3     2       0   3   0       0   \n","6    0   0    0     0       0       0    0    0     1       0   1   0       0   \n","\n","   meowing  once  orange  penny  saw  still  store  the  to  went  \n","0        0     0       0      1    0      0      0    0   0     0  \n","1        0     0       1      1    0      0      0    0   0     0  \n","2        0     0       0      0    0      0      1    2   0     0  \n","3        0     0       0      3    1      0      1    1   1     1  \n","4        1     1       0      0    0      1      0    3   0     0  \n","5        1     0       1      0    0      0      1    5   0     0  \n","6        0     0       0      1    0      0      0    0   0     0  "]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["# count_vectorizer.get_feature_names_out() returns the words that were tokenized\n","J = pd.DataFrame(X.toarray(), columns=count_vectorizer.get_feature_names_out())\n","# this returns a pandas dataframe. Dataframe(ndarray, columns=features)\n","# note that data frames are a two-dimensional, data structure with labeled axes (rows and columns). see data_analysis.ipynb for more.\n","J"]},{"cell_type":"markdown","metadata":{"id":"RV-ozKZeyxFn"},"source":["Note that any single letter words like \"a\" and \"I\" get excluded by `count_vectorizer`. If we examine the resultant matrix, we can see that there are many words which are relatively redundant and may not be useful when it comes to distinguishing meaning of the sentence (ex. and, at, it, etc.). We call these **stopwords** and we can remove the stopwords from the matrix when we vectorize our points."]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":163,"status":"ok","timestamp":1715300523907,"user":{"displayName":"Nakul Upadhya","userId":"08924826005411940959"},"user_tz":240},"id":"l8ZUfx_wyxFn","outputId":"ffd1ba9b-8ce0-434f-d643-1ed44aa5aa6a"},"outputs":[{"name":"stdout","output_type":"stream","text":["['ate' 'blue' 'bought' 'bright' 'bug' 'cat' 'fish' 'fishes' 'meowed'\n"," 'meowing' 'orange' 'penny' 'saw' 'store' 'went']\n"]}],"source":["# we'll make a new vectorizer\n","count_vectorizer = CountVectorizer(stop_words='english')\n","# count_vectorizer = CountVectorizer(stop_words=['the', 'and'])\n","\n","X = count_vectorizer.fit_transform(texts)\n","print(count_vectorizer.get_feature_names_out())"]},{"cell_type":"markdown","metadata":{},"source":["Note that `CounterVectorizer(stop_words='english')` will return a vectorizor that ignores stop words from English when tokenizing."]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":269},"executionInfo":{"elapsed":188,"status":"ok","timestamp":1715300525051,"user":{"displayName":"Nakul Upadhya","userId":"08924826005411940959"},"user_tz":240},"id":"FH1kDo7myxFn","outputId":"85756a1c-34a9-4bb9-a1b8-dc1b8d2ee994"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ate</th>\n","      <th>blue</th>\n","      <th>bought</th>\n","      <th>bright</th>\n","      <th>bug</th>\n","      <th>cat</th>\n","      <th>fish</th>\n","      <th>fishes</th>\n","      <th>meowed</th>\n","      <th>meowing</th>\n","      <th>orange</th>\n","      <th>penny</th>\n","      <th>saw</th>\n","      <th>store</th>\n","      <th>went</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   ate  blue  bought  bright  bug  cat  fish  fishes  meowed  meowing  orange  \\\n","0    0     1       1       1    0    0     0       1       0        0       0   \n","1    0     1       1       1    0    0     1       0       0        0       1   \n","2    1     0       0       0    0    1     1       0       0        0       0   \n","3    1     0       0       0    1    0     1       0       0        0       0   \n","4    0     0       0       0    2    0     1       0       1        1       0   \n","5    0     0       0       0    0    3     2       0       0        1       1   \n","6    0     0       0       0    0    0     1       0       0        0       0   \n","\n","   penny  saw  store  went  \n","0      1    0      0     0  \n","1      1    0      0     0  \n","2      0    0      1     0  \n","3      3    1      1     1  \n","4      0    0      0     0  \n","5      0    0      1     0  \n","6      1    0      0     0  "]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["pd.DataFrame(X.toarray(), columns=count_vectorizer.get_feature_names_out())"]},{"cell_type":"markdown","metadata":{"id":"IKYAsUv5yxFn"},"source":["I still see `meowed` and `meowing` and `fish` and `fishes` - they seem the same, so let's lemmatize/stem them, ie) combine them into one occurence. Before we do that, we will introduce making our own custom tokenizer.\n","\n","You can specify a `preprocessor` or a `tokenizer` when you're creating your `CountVectorizer` to do custom *stuff* on your words. Maybe we want to get rid of punctuation, lowercase things and split them on spaces (this is basically the default). `preprocessor` is supposed to return a string, so it's a little easier to work with."]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":185,"status":"ok","timestamp":1715300689151,"user":{"displayName":"Nakul Upadhya","userId":"08924826005411940959"},"user_tz":240},"id":"Q_CIWseVyxFn","outputId":"82021214-200c-49d1-8f46-53d3a26112e1"},"outputs":[{"name":"stdout","output_type":"stream","text":["['penny', 'went', 'to', 'the', 'store', 'penny', 'ate', 'a', 'bug', 'penny', 'saw', 'a', 'fish']\n","\n","\n","['ate' 'blue' 'bought' 'bright' 'bug' 'cat' 'fish' 'fishes' 'meowed'\n"," 'meowing' 'orange' 'penny' 'saw' 'store' 'went']\n"]},{"name":"stderr","output_type":"stream","text":["d:\\735-D\\code\\synced\\langs\\0_misc\\Learn-to-Code\\python\\env\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n","  warnings.warn(\n"]}],"source":["# This is what our normal tokenizer looks like\n","def boring_tokenizer(str_input):\n","    words = re.sub(r\"[^A-Za-z0-9\\-]\", \" \", str_input).lower().split()\n","    # re.sub(regex, replacement, string) replaces the regex in the string with the replacement.\n","    # so here, we're replacing all non-alphanumeric characters (and non-hypen -) with a space. \n","    return words\n","\n","# example use\n","print(boring_tokenizer(texts[3]))\n","print(\"\\n\")\n","\n","count_vectorizer = CountVectorizer(stop_words='english', tokenizer=boring_tokenizer)\n","X = count_vectorizer.fit_transform(texts)\n","\n","# this should return the same thing as the previous cell because the tokenizor is how counter_vectorizer behaves by default\n","print(count_vectorizer.get_feature_names_out())"]},{"cell_type":"markdown","metadata":{},"source":["To better understand how a tokenizer works, think of it like this: We call `count_vectorizer.fit_transform(str_list)` and `str_list` is our list of strings. Then, the tokenizer will be called on each element in `str_list`, and all the `stop_words` will be excluded. Then the filtered list of words will be used as columns and the occurence of each word in a sentence will be counted in the sparse array `X`."]},{"cell_type":"markdown","metadata":{"id":"lzOy8bT-yxFo"},"source":["Now we work on combining similar words together. We're going to use one that features a **stemmer** - something that strips the endings off of words (or tries to, at least). This one is from `nltk`."]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":177,"status":"ok","timestamp":1715300702391,"user":{"displayName":"Nakul Upadhya","userId":"08924826005411940959"},"user_tz":240},"id":"2LkJTSiIyxFo","outputId":"9862e2c3-e8e6-47b2-e1e5-33d20ca11716"},"outputs":[{"name":"stdout","output_type":"stream","text":["fish\n","meow\n","orang\n","meow\n","orang\n","go\n","went\n"]}],"source":["# https://tartarus.org/martin/PorterStemmer/def.txt\n","from nltk.stem.porter import PorterStemmer\n","porter_stemmer = PorterStemmer()\n","\n","print(porter_stemmer.stem('fishes'))\n","print(porter_stemmer.stem('meowed'))\n","print(porter_stemmer.stem('oranges'))\n","print(porter_stemmer.stem('meowing'))\n","print(porter_stemmer.stem('orange'))\n","print(porter_stemmer.stem('go'))\n","print(porter_stemmer.stem('went'))"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":229,"status":"ok","timestamp":1715300707712,"user":{"displayName":"Nakul Upadhya","userId":"08924826005411940959"},"user_tz":240},"id":"FijAd-2eyxFo","outputId":"47149212-d3d9-4b82-9767-55f25e93865b"},"outputs":[{"name":"stdout","output_type":"stream","text":["['ate' 'blue' 'bought' 'bright' 'bug' 'cat' 'fish' 'meow' 'onc' 'orang'\n"," 'penni' 'saw' 'store' 'went']\n"]},{"name":"stderr","output_type":"stream","text":["d:\\735-D\\code\\synced\\langs\\0_misc\\Learn-to-Code\\python\\env\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n","  warnings.warn(\n"]}],"source":["porter_stemmer = PorterStemmer()\n","\n","# same thing as boring_tokenizer, except we stem the words\n","def stemming_tokenizer(str_input):\n","    words = re.sub(r\"[^A-Za-z0-9\\-]\", \" \", str_input).lower().split()\n","    words = [porter_stemmer.stem(word) for word in words]\n","    return words\n","\n","count_vectorizer = CountVectorizer(stop_words='english', tokenizer=stemming_tokenizer)\n","X = count_vectorizer.fit_transform(texts)\n","print(count_vectorizer.get_feature_names_out())"]},{"cell_type":"markdown","metadata":{"id":"xocWpyAoyxFo"},"source":["Now lets look at the new version of that dataframe."]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":269},"executionInfo":{"elapsed":149,"status":"ok","timestamp":1715301039581,"user":{"displayName":"Nakul Upadhya","userId":"08924826005411940959"},"user_tz":240},"id":"FFHE-px2yxFo","outputId":"83b73a1f-eef7-432c-bb26-c2ca8468b032"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ate</th>\n","      <th>blue</th>\n","      <th>bought</th>\n","      <th>bright</th>\n","      <th>bug</th>\n","      <th>cat</th>\n","      <th>fish</th>\n","      <th>meow</th>\n","      <th>onc</th>\n","      <th>orang</th>\n","      <th>penni</th>\n","      <th>saw</th>\n","      <th>store</th>\n","      <th>went</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   ate  blue  bought  bright  bug  cat  fish  meow  onc  orang  penni  saw  \\\n","0    0     1       1       1    0    0     1     0    0      0      1    0   \n","1    0     1       1       1    0    0     1     0    0      1      1    0   \n","2    1     0       0       0    0    1     1     0    0      0      0    0   \n","3    1     0       0       0    1    0     1     0    0      0      3    1   \n","4    0     0       0       0    2    0     1     2    1      0      0    0   \n","5    0     0       0       0    0    3     2     1    0      1      0    0   \n","6    0     0       0       0    0    0     1     0    0      0      1    0   \n","\n","   store  went  \n","0      0     0  \n","1      0     0  \n","2      1     0  \n","3      1     1  \n","4      0     0  \n","5      1     0  \n","6      0     0  "]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["pd.DataFrame(X.toarray(), columns=count_vectorizer.get_feature_names_out())"]},{"cell_type":"markdown","metadata":{"id":"_QKy9VR3yxFo"},"source":["    \"Penny bought bright blue fishes.\",\n","    \"Penny bought bright blue and orange fish.\",\n","    \"The cat ate a fish at the store.\",\n","    \"Penny went to the store. Penny ate a bug. Penny saw a fish.\",\n","    \"It meowed once at the bug, it is still meowing at the bug and the fish\",\n","    \"The cat is at the fish store. The cat is orange. The cat is meowing at the fish.\",\n","    \"Penny is a fish\""]},{"cell_type":"markdown","metadata":{"id":"CB-xHn3MyxFo"},"source":["## TF-IDF\n","\n","### Part One: Term Frequency\n","\n","TF-IDF? What? It means **term frequency inverse document frequency!** It's the most important thing. Let's look at our list of phrases\n","\n","1. Penny bought bright blue fishes.\n","2. Penny bought bright blue and orange fish.\n","3. The cat ate a fish at the store.\n","4. Penny went to the store. Penny ate a bug. Penny saw a fish.\n","5. It meowed once at the fish, it is still meowing at the fish. It meowed at the bug and the fish.\n","6. The cat is fat. The cat is orange. The cat is meowing at the fish.\n","7. Penny is a fish\n","\n","If we are training a classifier to identify sentences about aquatic life, which is the most helpful phrase?"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":269},"executionInfo":{"elapsed":184,"status":"ok","timestamp":1715300723801,"user":{"displayName":"Nakul Upadhya","userId":"08924826005411940959"},"user_tz":240},"id":"CirrVc6XyxFo","outputId":"9bed8162-7eea-475d-a61c-59d80ee16380"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ate</th>\n","      <th>blue</th>\n","      <th>bought</th>\n","      <th>bright</th>\n","      <th>bug</th>\n","      <th>cat</th>\n","      <th>fish</th>\n","      <th>meow</th>\n","      <th>onc</th>\n","      <th>orang</th>\n","      <th>penni</th>\n","      <th>saw</th>\n","      <th>store</th>\n","      <th>went</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   ate  blue  bought  bright  bug  cat  fish  meow  onc  orang  penni  saw  \\\n","0    0     1       1       1    0    0     1     0    0      0      1    0   \n","1    0     1       1       1    0    0     1     0    0      1      1    0   \n","2    1     0       0       0    0    1     1     0    0      0      0    0   \n","3    1     0       0       0    1    0     1     0    0      0      3    1   \n","4    0     0       0       0    2    0     1     2    1      0      0    0   \n","5    0     0       0       0    0    3     2     1    0      1      0    0   \n","6    0     0       0       0    0    0     1     0    0      0      1    0   \n","\n","   store  went  \n","0      0     0  \n","1      0     0  \n","2      1     0  \n","3      1     1  \n","4      0     0  \n","5      1     0  \n","6      0     0  "]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["pd.DataFrame(X.toarray(), columns=count_vectorizer.get_feature_names_out())"]},{"cell_type":"markdown","metadata":{"id":"zm_dnfynyxFo"},"source":["Probably the one where `fish` appears two times.\n","\n","    It meowed once at the fish, it is still meowing at the fish. It meowed at the bug and the fish.\n","    \n","But are all the others the same?\n","\n","    Penny is a fish.\n","\n","    Penny went to the store. Penny ate a bug. Penny saw a fish.\n","\n","In the second one we spend less time talking about the fish. Think about a huge long document where they say your name once, versus a tweet where they say your name once. Which one are you more important in? Probably the tweet, since you take up a larger percentage of the text.\n","\n","This is **term frequency** - taking into account how often a term shows up. We're going to take this into account by using the `TfidfVectorizer` in the same way we used the `CountVectorizer`."]},{"cell_type":"code","execution_count":17,"metadata":{"collapsed":true,"id":"9-MjwrIZyxFo"},"outputs":[],"source":["from sklearn.feature_extraction.text import TfidfVectorizer"]},{"cell_type":"markdown","metadata":{},"source":["We're going to start by ignoring idf (inverse document frequency) for now. We do this by setting `use_idf=False`. "]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":358},"executionInfo":{"elapsed":205,"status":"ok","timestamp":1715301064207,"user":{"displayName":"Nakul Upadhya","userId":"08924826005411940959"},"user_tz":240},"id":"zvyJRG1LyxFo","outputId":"1da12a6a-c161-423d-d161-49d6f15707de"},"outputs":[{"name":"stderr","output_type":"stream","text":["d:\\735-D\\code\\synced\\langs\\0_misc\\Learn-to-Code\\python\\env\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n","  warnings.warn(\n","d:\\735-D\\code\\synced\\langs\\0_misc\\Learn-to-Code\\python\\env\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n","  warnings.warn(\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ate</th>\n","      <th>blue</th>\n","      <th>bought</th>\n","      <th>bright</th>\n","      <th>bug</th>\n","      <th>cat</th>\n","      <th>fish</th>\n","      <th>meow</th>\n","      <th>onc</th>\n","      <th>orang</th>\n","      <th>penni</th>\n","      <th>saw</th>\n","      <th>store</th>\n","      <th>went</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.000000</td>\n","      <td>0.200000</td>\n","      <td>0.200000</td>\n","      <td>0.200000</td>\n","      <td>0.000000</td>\n","      <td>0.000</td>\n","      <td>0.200000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.200000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.000000</td>\n","      <td>0.166667</td>\n","      <td>0.166667</td>\n","      <td>0.166667</td>\n","      <td>0.000000</td>\n","      <td>0.000</td>\n","      <td>0.166667</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.166667</td>\n","      <td>0.166667</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.250000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.250</td>\n","      <td>0.250000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.250000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.111111</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.111111</td>\n","      <td>0.000</td>\n","      <td>0.111111</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.333333</td>\n","      <td>0.111111</td>\n","      <td>0.111111</td>\n","      <td>0.111111</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.333333</td>\n","      <td>0.000</td>\n","      <td>0.166667</td>\n","      <td>0.333333</td>\n","      <td>0.166667</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.375</td>\n","      <td>0.250000</td>\n","      <td>0.125000</td>\n","      <td>0.000000</td>\n","      <td>0.125000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.125000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000</td>\n","      <td>0.500000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.500000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["        ate      blue    bought    bright       bug    cat      fish  \\\n","0  0.000000  0.200000  0.200000  0.200000  0.000000  0.000  0.200000   \n","1  0.000000  0.166667  0.166667  0.166667  0.000000  0.000  0.166667   \n","2  0.250000  0.000000  0.000000  0.000000  0.000000  0.250  0.250000   \n","3  0.111111  0.000000  0.000000  0.000000  0.111111  0.000  0.111111   \n","4  0.000000  0.000000  0.000000  0.000000  0.333333  0.000  0.166667   \n","5  0.000000  0.000000  0.000000  0.000000  0.000000  0.375  0.250000   \n","6  0.000000  0.000000  0.000000  0.000000  0.000000  0.000  0.500000   \n","\n","       meow       onc     orang     penni       saw     store      went  \n","0  0.000000  0.000000  0.000000  0.200000  0.000000  0.000000  0.000000  \n","1  0.000000  0.000000  0.166667  0.166667  0.000000  0.000000  0.000000  \n","2  0.000000  0.000000  0.000000  0.000000  0.000000  0.250000  0.000000  \n","3  0.000000  0.000000  0.000000  0.333333  0.111111  0.111111  0.111111  \n","4  0.333333  0.166667  0.000000  0.000000  0.000000  0.000000  0.000000  \n","5  0.125000  0.000000  0.125000  0.000000  0.000000  0.125000  0.000000  \n","6  0.000000  0.000000  0.000000  0.500000  0.000000  0.000000  0.000000  "]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["tfidf_vectorizer = TfidfVectorizer(stop_words='english', tokenizer=stemming_tokenizer, use_idf=False, norm='l1')\n","X = tfidf_vectorizer.fit_transform(texts)\n","pd.DataFrame(X.toarray(), columns=tfidf_vectorizer.get_feature_names_out())"]},{"cell_type":"markdown","metadata":{"id":"dAzgb4bcyxFp"},"source":["Now our numbers have shifted a little bit. Instead of just being a count, it's *the percentage of the words*.\n","\n","    value = (number of times word appears in sentence) / (number of words in sentence)\n","\n","After we remove the stopwords, the term `fish` is 50% of the words in `Penny is a fish` vs. 37.5% in `It meowed once at the fish, it is still meowing at the fish. It meowed at the bug and the fish.`.\n","\n","> **Note:** We made it be the percentage of the words by passing in `norm=\"l1\"` - by default it's normally an L2 (Euclidean) norm, which is actually better, but I thought it would make more sense starting by using the L1 - a.k.a. terms divided by words -norm."]},{"cell_type":"markdown","metadata":{"id":"Cgp-u42RyxFp"},"source":["So now when we train a classifier, it will be able to identify the important words easier because our pre-processing takes into account whether half of our words are `fish` or 1% of millions upon millions of words is `fish`. But we aren't done yet! We will now explore inverse document frequnecy as well."]},{"cell_type":"markdown","metadata":{"id":"UQ9lHo-TyxFp"},"source":["### Part Two: Inverse document frequency\n","\n","In addition to its importance to a given sentence, we also want to give information on how prevelant a given token is in the dataset as a whole. A token that is mentioned a lot in every dataset will probably be less important than a token that appears infrequently."]},{"cell_type":"markdown","metadata":{"id":"O-9NUlWByxFx"},"source":["This is **inverse document frequency** - the more often a term shows up across *all* documents, the less important it is in our matrix."]},{"cell_type":"code","execution_count":30,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":358},"executionInfo":{"elapsed":164,"status":"ok","timestamp":1715301069201,"user":{"displayName":"Nakul Upadhya","userId":"08924826005411940959"},"user_tz":240},"id":"_jaVRIPUyxFx","outputId":"3f72fe54-2d5a-47c6-dcd8-e4c76849e230"},"outputs":[{"name":"stderr","output_type":"stream","text":["d:\\735-D\\code\\synced\\langs\\0_misc\\Learn-to-Code\\python\\env\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n","  warnings.warn(\n","d:\\735-D\\code\\synced\\langs\\0_misc\\Learn-to-Code\\python\\env\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n","  warnings.warn(\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ate</th>\n","      <th>blue</th>\n","      <th>bought</th>\n","      <th>bright</th>\n","      <th>bug</th>\n","      <th>cat</th>\n","      <th>fish</th>\n","      <th>meow</th>\n","      <th>onc</th>\n","      <th>orang</th>\n","      <th>penni</th>\n","      <th>saw</th>\n","      <th>store</th>\n","      <th>went</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.000000</td>\n","      <td>0.235463</td>\n","      <td>0.235463</td>\n","      <td>0.235463</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.118871</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.174741</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.000000</td>\n","      <td>0.190587</td>\n","      <td>0.190587</td>\n","      <td>0.190587</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.096216</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.190587</td>\n","      <td>0.141437</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.297654</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.297654</td>\n","      <td>0.150267</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.254425</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.125073</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.125073</td>\n","      <td>0.000000</td>\n","      <td>0.063142</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.278455</td>\n","      <td>0.150675</td>\n","      <td>0.106908</td>\n","      <td>0.150675</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.350291</td>\n","      <td>0.000000</td>\n","      <td>0.088420</td>\n","      <td>0.350291</td>\n","      <td>0.210997</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.437035</td>\n","      <td>0.147088</td>\n","      <td>0.145678</td>\n","      <td>0.000000</td>\n","      <td>0.145678</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.124521</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.404858</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.595142</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["        ate      blue    bought    bright       bug       cat      fish  \\\n","0  0.000000  0.235463  0.235463  0.235463  0.000000  0.000000  0.118871   \n","1  0.000000  0.190587  0.190587  0.190587  0.000000  0.000000  0.096216   \n","2  0.297654  0.000000  0.000000  0.000000  0.000000  0.297654  0.150267   \n","3  0.125073  0.000000  0.000000  0.000000  0.125073  0.000000  0.063142   \n","4  0.000000  0.000000  0.000000  0.000000  0.350291  0.000000  0.088420   \n","5  0.000000  0.000000  0.000000  0.000000  0.000000  0.437035  0.147088   \n","6  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.404858   \n","\n","       meow       onc     orang     penni       saw     store      went  \n","0  0.000000  0.000000  0.000000  0.174741  0.000000  0.000000  0.000000  \n","1  0.000000  0.000000  0.190587  0.141437  0.000000  0.000000  0.000000  \n","2  0.000000  0.000000  0.000000  0.000000  0.000000  0.254425  0.000000  \n","3  0.000000  0.000000  0.000000  0.278455  0.150675  0.106908  0.150675  \n","4  0.350291  0.210997  0.000000  0.000000  0.000000  0.000000  0.000000  \n","5  0.145678  0.000000  0.145678  0.000000  0.000000  0.124521  0.000000  \n","6  0.000000  0.000000  0.000000  0.595142  0.000000  0.000000  0.000000  "]},"execution_count":30,"metadata":{},"output_type":"execute_result"}],"source":["# use_idf=True is default, but we'll include it here for clarity\n","idf_vectorizer = TfidfVectorizer(stop_words='english', tokenizer=stemming_tokenizer, use_idf=True, norm='l1')\n","X = idf_vectorizer.fit_transform(texts)\n","idf_df = pd.DataFrame(X.toarray(), columns=idf_vectorizer.get_feature_names_out())\n","idf_df"]},{"cell_type":"markdown","metadata":{},"source":["Note that this time we enter `use_idf=True`. This is already True by default but it's added in for emphasis as we had set it to False before."]},{"cell_type":"markdown","metadata":{"id":"UiiieSOAyxFy"},"source":["Notice how 'meow' increased in value because it's an infrequent term, and `fish` dropped in value because it's so frequent.\n","\n","That meowing one (index 4) has gone from `0.50` to `0.43`, while `Penny is a fish` (index 6) has dropped to `0.40`. Now hooray, the meowing one is going to show up earlier when searching for \"fish meow\" because *fish shows up all of the time, so we want to ignore it a lil' bit*.\n","\n","Let's try changing it to `norm='l2'` (or just removing `norm` completely) to see how this might change our matrix."]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":358},"executionInfo":{"elapsed":149,"status":"ok","timestamp":1715301072330,"user":{"displayName":"Nakul Upadhya","userId":"08924826005411940959"},"user_tz":240},"id":"Xz9TbzxyyxFy","outputId":"1228ab9c-cd52-476b-d5fd-d7e1c008b7d2"},"outputs":[{"name":"stderr","output_type":"stream","text":["d:\\735-D\\code\\synced\\langs\\0_misc\\Learn-to-Code\\python\\env\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n","  warnings.warn(\n","d:\\735-D\\code\\synced\\langs\\0_misc\\Learn-to-Code\\python\\env\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n","  warnings.warn(\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ate</th>\n","      <th>blue</th>\n","      <th>bought</th>\n","      <th>bright</th>\n","      <th>bug</th>\n","      <th>cat</th>\n","      <th>fish</th>\n","      <th>meow</th>\n","      <th>onc</th>\n","      <th>orang</th>\n","      <th>penni</th>\n","      <th>saw</th>\n","      <th>store</th>\n","      <th>went</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.000000</td>\n","      <td>0.512612</td>\n","      <td>0.512612</td>\n","      <td>0.512612</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.258786</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.380417</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.000000</td>\n","      <td>0.456170</td>\n","      <td>0.456170</td>\n","      <td>0.456170</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.230292</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.456170</td>\n","      <td>0.338530</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.578752</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.578752</td>\n","      <td>0.292176</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.494698</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.303663</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.303663</td>\n","      <td>0.000000</td>\n","      <td>0.153301</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.676058</td>\n","      <td>0.365821</td>\n","      <td>0.259561</td>\n","      <td>0.365821</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.641958</td>\n","      <td>0.000000</td>\n","      <td>0.162043</td>\n","      <td>0.641958</td>\n","      <td>0.386682</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.840166</td>\n","      <td>0.282766</td>\n","      <td>0.280055</td>\n","      <td>0.000000</td>\n","      <td>0.280055</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.239382</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.562463</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.826823</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["        ate      blue    bought    bright       bug       cat      fish  \\\n","0  0.000000  0.512612  0.512612  0.512612  0.000000  0.000000  0.258786   \n","1  0.000000  0.456170  0.456170  0.456170  0.000000  0.000000  0.230292   \n","2  0.578752  0.000000  0.000000  0.000000  0.000000  0.578752  0.292176   \n","3  0.303663  0.000000  0.000000  0.000000  0.303663  0.000000  0.153301   \n","4  0.000000  0.000000  0.000000  0.000000  0.641958  0.000000  0.162043   \n","5  0.000000  0.000000  0.000000  0.000000  0.000000  0.840166  0.282766   \n","6  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.562463   \n","\n","       meow       onc     orang     penni       saw     store      went  \n","0  0.000000  0.000000  0.000000  0.380417  0.000000  0.000000  0.000000  \n","1  0.000000  0.000000  0.456170  0.338530  0.000000  0.000000  0.000000  \n","2  0.000000  0.000000  0.000000  0.000000  0.000000  0.494698  0.000000  \n","3  0.000000  0.000000  0.000000  0.676058  0.365821  0.259561  0.365821  \n","4  0.641958  0.386682  0.000000  0.000000  0.000000  0.000000  0.000000  \n","5  0.280055  0.000000  0.280055  0.000000  0.000000  0.239382  0.000000  \n","6  0.000000  0.000000  0.000000  0.826823  0.000000  0.000000  0.000000  "]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["# norm gets set to l2 by default\n","l2_vectorizer = TfidfVectorizer(stop_words='english', tokenizer=stemming_tokenizer, use_idf=True)\n","X = l2_vectorizer.fit_transform(texts)\n","l2_df = pd.DataFrame(X.toarray(), columns=l2_vectorizer.get_feature_names_out())\n","l2_df"]},{"cell_type":"markdown","metadata":{},"source":["Note that our values all went up drastically."]},{"cell_type":"markdown","metadata":{"id":"-6d0s3sMyxFy"},"source":["## Real Life Example\n","\n","Lets take what we learned and apply it to a real life classification task. AG is a collection of more than 1 million news articles organized into four different topics: Business, Tech, Sports, and World News.\n","Our task is to:\n","1. Apply the text pre-processing you learnt above to get a clean dataset.\n","2. Train a classifier on your processed data.\n","3. Report the accuracy, precision, recall, and F1 (there are multiple classes so think about which F1 to report)\n","\n","Code to download and sample the data is already provided.\n","\n"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7917,"status":"ok","timestamp":1715302104065,"user":{"displayName":"Nakul Upadhya","userId":"08924826005411940959"},"user_tz":240},"id":"vfjZOGZF9RUV","outputId":"e26218fe-1e06-4dbf-c516-84cddf13c23d"},"outputs":[{"name":"stdout","output_type":"stream","text":["(2000, 2)\n","(500, 2)\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Wall St. Bears Claw Back Into the Black (Reute...</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Carlyle Looks Toward Commercial Aerospace (Reu...</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Oil and Economy Cloud Stocks' Outlook (Reuters...</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Iraq Halts Oil Exports from Main Southern Pipe...</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Oil prices soar to all-time record, posing new...</td>\n","      <td>2</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                text  label\n","0  Wall St. Bears Claw Back Into the Black (Reute...      2\n","1  Carlyle Looks Toward Commercial Aerospace (Reu...      2\n","2  Oil and Economy Cloud Stocks' Outlook (Reuters...      2\n","3  Iraq Halts Oil Exports from Main Southern Pipe...      2\n","4  Oil prices soar to all-time record, posing new...      2"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["from datasets import load_dataset\n","\n","dataset = load_dataset(\"ag_news\", cache_dir='data/datasets/text/ag_news')\n","train = dataset['train']\n","test = dataset['test']\n","# convert to pandas\n","train = pd.DataFrame(train).head(2000)\n","test = pd.DataFrame(test).head(500)\n","print(train.shape)\n","print(test.shape)\n","train.head()"]},{"cell_type":"markdown","metadata":{},"source":["Note that train is our training dataset and test is ourr testing data set."]},{"cell_type":"code","execution_count":36,"metadata":{"id":"CdtRBJKi_ay-"},"outputs":[{"name":"stderr","output_type":"stream","text":["d:\\735-D\\code\\synced\\langs\\0_misc\\Learn-to-Code\\python\\env\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n","  warnings.warn(\n","d:\\735-D\\code\\synced\\langs\\0_misc\\Learn-to-Code\\python\\env\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n","  warnings.warn(\n"]}],"source":["train_texts = train['text'] # get the column (this is a Pandas Series)\n","test_texts = test['text'] \n","\n","# construct our vectorizer just like before with the same tokenizer. recall the tokenizer just filters our words\n","tfidf_vectorizer = TfidfVectorizer(stop_words='english', \n","                                   tokenizer=stemming_tokenizer, \n","                                   use_idf=True, \n","                                   norm='l2',\n","                                   min_df=0.01,\n","                                   max_df=0.99)\n","# set min_df and max_df to filter out words that appear in less than 1% of the documents or more than 99% of the documents\n","\n","X_train = tfidf_vectorizer.fit_transform(train_texts).toarray()\n","\n","# this time we use tfidf_vectorizer.transform()\n","# we apply a vectorization to the test data using the IDFs for each word which was obtained from the train data\n","X_test = tfidf_vectorizer.transform(test_texts).toarray()\n","\n","\n","Y_train = train['label']\n","Y_test = test['label']\n","\n","train_df = pd.DataFrame(data=X_train, columns=tfidf_vectorizer.get_feature_names_out())\n","test_df = pd.DataFrame(data=X_test, columns=tfidf_vectorizer.get_feature_names_out())"]},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>-</th>\n","      <th>--</th>\n","      <th>0</th>\n","      <th>000</th>\n","      <th>1</th>\n","      <th>10</th>\n","      <th>100</th>\n","      <th>15</th>\n","      <th>16</th>\n","      <th>17</th>\n","      <th>2</th>\n","      <th>2004</th>\n","      <th>3</th>\n","      <th>36</th>\n","      <th>39</th>\n","      <th>...</th>\n","      <th>week</th>\n","      <th>west</th>\n","      <th>win</th>\n","      <th>window</th>\n","      <th>wireless</th>\n","      <th>women</th>\n","      <th>won</th>\n","      <th>work</th>\n","      <th>world</th>\n","      <th>worri</th>\n","      <th>www</th>\n","      <th>xp</th>\n","      <th>year</th>\n","      <th>yesterday</th>\n","      <th>york</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.135449</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.101354</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.073639</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.173042</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.224442</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.082500</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.078498</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.172364</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1995</th>\n","      <td>0.090436</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>1996</th>\n","      <td>0.137244</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>1997</th>\n","      <td>0.109065</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>1998</th>\n","      <td>0.123393</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>1999</th>\n","      <td>0.079006</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.206887</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.176231</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2000 rows × 480 columns</p>\n","</div>"],"text/plain":["             -   --    0  000    1   10  100   15   16   17         2  2004  \\\n","0     0.135449  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.000000   0.0   \n","1     0.101354  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.000000   0.0   \n","2     0.073639  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.000000   0.0   \n","3     0.082500  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.000000   0.0   \n","4     0.078498  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.000000   0.0   \n","...        ...  ...  ...  ...  ...  ...  ...  ...  ...  ...       ...   ...   \n","1995  0.090436  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.000000   0.0   \n","1996  0.137244  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.000000   0.0   \n","1997  0.109065  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.000000   0.0   \n","1998  0.123393  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.000000   0.0   \n","1999  0.079006  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.206887   0.0   \n","\n","        3   36   39  ...      week  west  win  window  wireless  women  won  \\\n","0     0.0  0.0  0.0  ...  0.000000   0.0  0.0     0.0       0.0    0.0  0.0   \n","1     0.0  0.0  0.0  ...  0.000000   0.0  0.0     0.0       0.0    0.0  0.0   \n","2     0.0  0.0  0.0  ...  0.173042   0.0  0.0     0.0       0.0    0.0  0.0   \n","3     0.0  0.0  0.0  ...  0.000000   0.0  0.0     0.0       0.0    0.0  0.0   \n","4     0.0  0.0  0.0  ...  0.000000   0.0  0.0     0.0       0.0    0.0  0.0   \n","...   ...  ...  ...  ...       ...   ...  ...     ...       ...    ...  ...   \n","1995  0.0  0.0  0.0  ...  0.000000   0.0  0.0     0.0       0.0    0.0  0.0   \n","1996  0.0  0.0  0.0  ...  0.000000   0.0  0.0     0.0       0.0    0.0  0.0   \n","1997  0.0  0.0  0.0  ...  0.000000   0.0  0.0     0.0       0.0    0.0  0.0   \n","1998  0.0  0.0  0.0  ...  0.000000   0.0  0.0     0.0       0.0    0.0  0.0   \n","1999  0.0  0.0  0.0  ...  0.000000   0.0  0.0     0.0       0.0    0.0  0.0   \n","\n","      work     world     worri  www   xp  year  yesterday      york  \n","0      0.0  0.000000  0.000000  0.0  0.0   0.0        0.0  0.000000  \n","1      0.0  0.000000  0.000000  0.0  0.0   0.0        0.0  0.000000  \n","2      0.0  0.000000  0.224442  0.0  0.0   0.0        0.0  0.000000  \n","3      0.0  0.000000  0.000000  0.0  0.0   0.0        0.0  0.000000  \n","4      0.0  0.172364  0.000000  0.0  0.0   0.0        0.0  0.000000  \n","...    ...       ...       ...  ...  ...   ...        ...       ...  \n","1995   0.0  0.000000  0.000000  0.0  0.0   0.0        0.0  0.000000  \n","1996   0.0  0.000000  0.000000  0.0  0.0   0.0        0.0  0.000000  \n","1997   0.0  0.000000  0.000000  0.0  0.0   0.0        0.0  0.000000  \n","1998   0.0  0.000000  0.000000  0.0  0.0   0.0        0.0  0.000000  \n","1999   0.0  0.000000  0.000000  0.0  0.0   0.0        0.0  0.176231  \n","\n","[2000 rows x 480 columns]"]},"execution_count":37,"metadata":{},"output_type":"execute_result"}],"source":["train_df"]},{"cell_type":"code","execution_count":38,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>-</th>\n","      <th>--</th>\n","      <th>0</th>\n","      <th>000</th>\n","      <th>1</th>\n","      <th>10</th>\n","      <th>100</th>\n","      <th>15</th>\n","      <th>16</th>\n","      <th>17</th>\n","      <th>2</th>\n","      <th>2004</th>\n","      <th>3</th>\n","      <th>36</th>\n","      <th>39</th>\n","      <th>...</th>\n","      <th>week</th>\n","      <th>west</th>\n","      <th>win</th>\n","      <th>window</th>\n","      <th>wireless</th>\n","      <th>women</th>\n","      <th>won</th>\n","      <th>work</th>\n","      <th>world</th>\n","      <th>worri</th>\n","      <th>www</th>\n","      <th>xp</th>\n","      <th>year</th>\n","      <th>yesterday</th>\n","      <th>york</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.065555</td>\n","      <td>0.130290</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.192245</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.199803</td>\n","      <td>0.000000</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.105762</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.241596</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.265344</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.098530</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.130323</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>495</th>\n","      <td>0.000000</td>\n","      <td>0.144059</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.212561</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>496</th>\n","      <td>0.000000</td>\n","      <td>0.144241</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.202776</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>497</th>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>498</th>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>499</th>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.550378</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>500 rows × 480 columns</p>\n","</div>"],"text/plain":["            -        --    0  000    1        10  100   15   16   17    2  \\\n","0    0.000000  0.000000  0.0  0.0  0.0  0.000000  0.0  0.0  0.0  0.0  0.0   \n","1    0.065555  0.130290  0.0  0.0  0.0  0.192245  0.0  0.0  0.0  0.0  0.0   \n","2    0.105762  0.000000  0.0  0.0  0.0  0.000000  0.0  0.0  0.0  0.0  0.0   \n","3    0.098530  0.000000  0.0  0.0  0.0  0.000000  0.0  0.0  0.0  0.0  0.0   \n","4    0.130323  0.000000  0.0  0.0  0.0  0.000000  0.0  0.0  0.0  0.0  0.0   \n","..        ...       ...  ...  ...  ...       ...  ...  ...  ...  ...  ...   \n","495  0.000000  0.144059  0.0  0.0  0.0  0.212561  0.0  0.0  0.0  0.0  0.0   \n","496  0.000000  0.144241  0.0  0.0  0.0  0.000000  0.0  0.0  0.0  0.0  0.0   \n","497  0.000000  0.000000  0.0  0.0  0.0  0.000000  0.0  0.0  0.0  0.0  0.0   \n","498  0.000000  0.000000  0.0  0.0  0.0  0.000000  0.0  0.0  0.0  0.0  0.0   \n","499  0.000000  0.000000  0.0  0.0  0.0  0.000000  0.0  0.0  0.0  0.0  0.0   \n","\n","     2004    3        36        39  ...  week  west       win  window  \\\n","0     0.0  0.0  0.000000  0.000000  ...   0.0   0.0  0.000000     0.0   \n","1     0.0  0.0  0.199803  0.000000  ...   0.0   0.0  0.000000     0.0   \n","2     0.0  0.0  0.000000  0.000000  ...   0.0   0.0  0.241596     0.0   \n","3     0.0  0.0  0.000000  0.000000  ...   0.0   0.0  0.000000     0.0   \n","4     0.0  0.0  0.000000  0.000000  ...   0.0   0.0  0.000000     0.0   \n","..    ...  ...       ...       ...  ...   ...   ...       ...     ...   \n","495   0.0  0.0  0.000000  0.000000  ...   0.0   0.0  0.000000     0.0   \n","496   0.0  0.0  0.000000  0.000000  ...   0.0   0.0  0.000000     0.0   \n","497   0.0  0.0  0.000000  0.000000  ...   0.0   0.0  0.000000     0.0   \n","498   0.0  0.0  0.000000  0.000000  ...   0.0   0.0  0.000000     0.0   \n","499   0.0  0.0  0.000000  0.550378  ...   0.0   0.0  0.000000     0.0   \n","\n","     wireless  women       won  work  world  worri  www   xp  year  yesterday  \\\n","0         0.0    0.0  0.000000   0.0    0.0    0.0  0.0  0.0   0.0   0.000000   \n","1         0.0    0.0  0.000000   0.0    0.0    0.0  0.0  0.0   0.0   0.000000   \n","2         0.0    0.0  0.265344   0.0    0.0    0.0  0.0  0.0   0.0   0.000000   \n","3         0.0    0.0  0.000000   0.0    0.0    0.0  0.0  0.0   0.0   0.000000   \n","4         0.0    0.0  0.000000   0.0    0.0    0.0  0.0  0.0   0.0   0.000000   \n","..        ...    ...       ...   ...    ...    ...  ...  ...   ...        ...   \n","495       0.0    0.0  0.000000   0.0    0.0    0.0  0.0  0.0   0.0   0.000000   \n","496       0.0    0.0  0.000000   0.0    0.0    0.0  0.0  0.0   0.0   0.202776   \n","497       0.0    0.0  0.000000   0.0    0.0    0.0  0.0  0.0   0.0   0.000000   \n","498       0.0    0.0  0.000000   0.0    0.0    0.0  0.0  0.0   0.0   0.000000   \n","499       0.0    0.0  0.000000   0.0    0.0    0.0  0.0  0.0   0.0   0.000000   \n","\n","     york  \n","0     0.0  \n","1     0.0  \n","2     0.0  \n","3     0.0  \n","4     0.0  \n","..    ...  \n","495   0.0  \n","496   0.0  \n","497   0.0  \n","498   0.0  \n","499   0.0  \n","\n","[500 rows x 480 columns]"]},"execution_count":38,"metadata":{},"output_type":"execute_result"}],"source":["test_df"]},{"cell_type":"markdown","metadata":{},"source":["We have finished cleaning up our training texts. Now we will train a classifier on this data. We will use `xgboost` libraryr which is provides a gradient boosting framework. \n","\n","### Gradient Boosting\n","Gradient boosting is an ML technique thaat produces a prediction model in the form of an esemble of weak prerdiction models, typically decision trees. It builds the model in a stage-wise fashion and generalizes them by allowing optimization of abitrary differnetiable loss function.\n","\n","XGBoost is a go-to library for high performance machine learning tasks, especially where accuracy is a critical criterion. It supports various objection functions, including regression, classification, and ranking."]},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[],"source":["from xgboost import XGBClassifier\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.metrics import make_scorer, accuracy_score, f1_score, precision_score, recall_score"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["hyperparameter_search = {\n","    'max_depth': [2,6], \n","    'lambda': [0,0, 1e-3],\n","    'alpha': [0,0,1e-3] \n","}\n","\n","evaluation_metric = make_scorer(accuracy_score, greater_is_better=True)"]}],"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/littlecolumns/ds4j-notebooks/blob/master/text-analysis/notebooks/A%20simple%20explanation%20of%20TF-IDF.ipynb","timestamp":1715299097964}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.1"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false}},"nbformat":4,"nbformat_minor":0}
